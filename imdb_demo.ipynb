{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from imdbNet.imdb_utils import load_imdb, imdb_word_dic, pad_data\nfrom imdbNet.dataGenerator import DataGenerator\nfrom imdbNet.imdbModel import imdbModel\nfrom imdbNet.train import Train\nimport tensorflow as tf",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# train parameter\nhparam = {\n    'train_size': 2000,\n    'valid_size': 500,\n    'test_size': 500,\n    'batch_size': 32,\n    'num_epochs': 10,\n    'num_steps': 100,\n    'hidden_size1': 16,\n    'hidden_size2': 32,\n    'output_size': 2,\n    'keep_prob': 0.5,\n    'vocab_size': 10000,\n    'embedding_dim': 50,\n    'max_grad_norm': 2\n    }\n\n# load data\ntrain_x, train_y, valid_x, valid_y, test_x, test_y = load_imdb(num_training=hparam['train_size'],\n                                                               num_validation=hparam['valid_size'],\n                                                               num_test=hparam['test_size'],\n                                                               num_words=hparam['vocab_size'])\nword2index, index2word = imdb_word_dic()\n\n# Pad data\ntrain_x, valid_x, test_x = pad_data(train_x, valid_x, test_x, word2index, max_len=hparam['num_steps'])\nprint(f\"\\ntrain_x: {train_x.shape}  valid_x: {valid_x.shape} test_x: {test_x.shape}\")\nprint(f\"train_y: {train_y.shape} valid_y: {valid_y.shape} test_y: {test_y.shape}\")",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "\ntrain_x: (2000, 100)  valid_x: (500, 100) test_x: (500, 100)\ntrain_y: (2000,) valid_y: (500,) test_y: (500,)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()\n# Define Models\nwith tf.variable_scope(\"model\", reuse=None):\n    train_model = imdbModel(\"is_training\", hparam['num_steps'], hparam['hidden_size1'],hparam['hidden_size2'],\n                            hparam['output_size'], hparam['vocab_size'], hparam['output_size'],\n                            hparam['max_grad_norm'], hparam['keep_prob'])\n\nwith tf.variable_scope(\"model\", reuse=True):\n    valid_model = imdbModel(\"is_valid\", hparam['num_steps'], hparam['hidden_size1'],hparam['hidden_size2'],\n                            hparam['output_size'], hparam['vocab_size'], hparam['output_size'],\n                            hparam['max_grad_norm'], keep_prob=1.0)\n\n    test_model = imdbModel(\"is_testing\", hparam['num_steps'], hparam['hidden_size1'],hparam['hidden_size2'],\n                            hparam['output_size'], hparam['vocab_size'], hparam['output_size'],\n                            hparam['max_grad_norm'], keep_prob=1.0)\n\n# Data Generators\ntrainGenerator = DataGenerator(train_x, train_y, batch_size=hparam['batch_size'], shuffle=True)\nvalidGenerator = DataGenerator(valid_x, valid_y, batch_size=hparam['batch_size'], shuffle=False)\ntestGenerator = DataGenerator(test_x, test_y, batch_size=hparam['batch_size'], shuffle=False)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Train Model\nwith tf.Session() as sess:\n    train = Train(session=sess, num_epochs=hparam['num_epochs'])\n    train(trainGenerator, train_model,\n          validGenerator, valid_model, verbose=False)\n\n    print(\"Predictions: \\n\")\n    train.predict(testGenerator, test_model)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "epoch: 1/10  train loss: 0.6926 valid loss: 0.6927  time: 4.80 secs\nepoch: 2/10  train loss: 0.6654 valid loss: 0.6806  time: 4.92 secs\nepoch: 3/10  train loss: 0.4633 valid loss: 0.7533  time: 4.61 secs\nepoch: 4/10  train loss: 0.2871 valid loss: 0.9521  time: 4.61 secs\nepoch: 5/10  train loss: 0.1691 valid loss: 1.0063  time: 4.55 secs\nepoch: 6/10  train loss: 0.1389 valid loss: 1.5289  time: 4.61 secs\nepoch: 7/10  train loss: 0.0950 valid loss: 1.7052  time: 4.31 secs\nepoch: 8/10  train loss: 0.0465 valid loss: 1.5604  time: 4.32 secs\nepoch: 9/10  train loss: 0.0263 valid loss: 1.9200  time: 4.68 secs\nepoch: 10/10  train loss: 0.0096 valid loss: 2.2250  time: 4.51 secs\nPredictions: \n\n321 / 500 correct 64.20\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}